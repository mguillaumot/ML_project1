{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# from costs import compute_mse, compute_loss\n",
    "# from plots import *\n",
    "from helpers import *\n",
    "# from grid_search import *\n",
    "# import datetime\n",
    "# from ridge_regression import *\n",
    "# from gradient_descent import *\n",
    "# from stochastic_gradient_descent import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_TRAIN = '../data/train.csv'\n",
    "# y_train, data_train, ids_tr = load_csv_data(DATA_PATH_TRAIN, sub_sample=True)\n",
    "y_train, data_train, ids_tr = load_csv_data(DATA_PATH_TRAIN, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_TEST = '../data/test.csv'\n",
    "_, data_test, ids_te = load_csv_data(DATA_PATH_TEST, sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(11365, 30)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prisgdd\\Documents\\EPFL\\MachineLearning\\projects\\ML_project1\\scripts\\helpers.py:210: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  def divide_subset(data, y = None):\n"
     ]
    }
   ],
   "source": [
    "data_train= put_NaN(data_train)\n",
    "datasets_train = divide_subset(data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = put_NaN(data_test)\n",
    "datasets_test = divide_subset(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, before preprocessing : (99913, 30) \n",
      "Train, after preprocessing : (99913, 18) \n",
      "Train, before preprocessing : (77544, 30) \n",
      "Train, after preprocessing : (77544, 21) \n",
      "Train, before preprocessing : (72543, 30) \n",
      "Train, after preprocessing : (72543, 30) \n",
      "\n",
      "Test, before preprocessing : (4496, 30) \n",
      "Test, after preprocessing : (4496, 18) \n",
      "Test, before preprocessing : (3525, 30) \n",
      "Test, after preprocessing : (3525, 21) \n",
      "Test, before preprocessing : (3344, 30) \n",
      "Test, after preprocessing : (3344, 30) \n"
     ]
    }
   ],
   "source": [
    "test = True\n",
    "# test = False\n",
    "\n",
    "# Preprocess datasets_train\n",
    "for ind, subset_train in enumerate(datasets_train):\n",
    "    print('Train, before preprocessing : {} '.format(subset_train[0].shape))\n",
    "    if ind == 0:\n",
    "        subset_train[0] = preprocess_data_train(subset_train[0], jet = 0)\n",
    "    elif ind == 1:\n",
    "        subset_train[0] = preprocess_data_train(subset_train[0], jet = 1)\n",
    "    elif ind == 2:\n",
    "        subset_train[0] = preprocess_data_train(subset_train[0], jet = 23)\n",
    "\n",
    "    print('Train, after preprocessing : {} '.format(subset_train[0].shape))\n",
    "    datasets_train[ind][0] = subset_train[0]\n",
    "if test == True:\n",
    "    print(\"\")\n",
    "    # Preprocess datasets_train\n",
    "    for ind, subset_test in enumerate(datasets_test):\n",
    "        subset_train = datasets_train[ind][0]\n",
    "\n",
    "        print('Test, before preprocessing : {} '.format(subset_test[0].shape))\n",
    "\n",
    "        if ind == 0:\n",
    "            subset_test[0] = preprocess_data_test(subset_test[0], subset_train, jet = 0)\n",
    "        elif ind == 1:\n",
    "            subset_test[0] = preprocess_data_test(subset_test[0], subset_train, jet = 1)\n",
    "        elif ind == 2:\n",
    "            subset_test[0] = preprocess_data_test(subset_test[0], subset_train, jet = 23)\n",
    "\n",
    "        print('Test, after preprocessing : {} '.format(subset_test[0].shape))\n",
    "        datasets_test[ind][0] = subset_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Least Square Gradient Descent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, method, max_iters = 0, gamma = 0, lambda_ = 0):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    # get k'th subgroup in test, others in train\n",
    "    ind_train = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    ind_test = k_indices[k]\n",
    "    ind_train = ind_train.reshape(-1)\n",
    "    \n",
    "    x_train = x[ind_train]\n",
    "    y_train = y[ind_train]\n",
    "    x_test = x[ind_test]\n",
    "    y_test = y[ind_test]\n",
    "    \n",
    "    # form data  -- TODO: Change for augmented features\n",
    "    _, poly_train = build_model_data(x_train, y_train)\n",
    "    _, poly_test = build_model_data(x_test, y_test)\n",
    "\n",
    "    if method == least_squares_GD or method == least_squares_SGD or method == logistic_regression:\n",
    "        initial_w = np.zeros([poly_train.shape[1]])\n",
    "        w, loss = method(y = y_train, tx = poly_train, initial_w = initial_w, max_iters = max_iters, gamma = gamma)\n",
    "        \n",
    "    elif method == least_squares:\n",
    "        w, loss = method(y = y_train, tx = poly_train)\n",
    "        \n",
    "    elif method == ridge_regression:\n",
    "        w, loss = method(y = y_train, tx = poly_train, lambda_ = lambda_)\n",
    "        \n",
    "    elif method == reg_logistic_regression:\n",
    "        initial_w = np.zeros([poly_train.shape[1]])\n",
    "        w, loss = method(y = y_train, tx = poly_train, lambda_ = lambda_, initial_w = initial_w, max_iters = max_iters, gamma = gamma)\n",
    "    \n",
    "    # Compute prediction for train and test\n",
    "    y_pred_train = predict_labels(w, poly_train)\n",
    "    y_pred_test = predict_labels(w, poly_test)\n",
    "    \n",
    "#     print(\" --- y = -1 :: Reel: {} :: Prediction: {}\".format( (y_train == -1).sum(), (y_pred_train == -1).sum() )) \n",
    "#     print(\" --- y = +1 :: Reel: {} :: Prediction: {}\".format( (y_train == 1).sum(), (y_pred_train == 1).sum() )) \n",
    "    \n",
    "    # Compute accuracy for train and test \n",
    "    accuracy_train = compute_accuracy(y_pred_train, y_train)\n",
    "    accuracy_test = compute_accuracy(y_pred_test, y_test)\n",
    "    \n",
    "    return accuracy_train, accuracy_test, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " For case jet = 0 \n",
      "\n",
      "\n",
      "lambda_ = 1\n",
      "gamma = 0.4 \n",
      "\n",
      "0 - Training accuracy: 0.817050 / Test accuracy : 0.817119\n",
      "1 - Training accuracy: 0.817508 / Test accuracy : 0.810553\n",
      "2 - Training accuracy: 0.817119 / Test accuracy : 0.814637\n",
      "3 - Training accuracy: 0.817588 / Test accuracy : 0.813516\n",
      "4 - Training accuracy: 0.816650 / Test accuracy : 0.823845\n",
      "5 - Training accuracy: 0.816753 / Test accuracy : 0.820802\n",
      "6 - Training accuracy: 0.816879 / Test accuracy : 0.815197\n",
      "7 - Training accuracy: 0.816856 / Test accuracy : 0.818160\n",
      "\n",
      "Average test accuracy: 0.8167287212747217\n",
      "Variance test accuracy: 0.0\n",
      "Min test accuracy: 0.8167287212747217\n",
      "Max test accuracy: 0.8167287212747217\n",
      "\n",
      "\n",
      "lambda_ = 1\n",
      "gamma = 0.5 \n",
      "\n",
      "0 - Training accuracy: 0.817062 / Test accuracy : 0.816959\n",
      "1 - Training accuracy: 0.817577 / Test accuracy : 0.811034\n",
      "2 - Training accuracy: 0.817188 / Test accuracy : 0.814557\n",
      "3 - Training accuracy: 0.817542 / Test accuracy : 0.813516\n",
      "4 - Training accuracy: 0.816673 / Test accuracy : 0.823365\n",
      "5 - Training accuracy: 0.816890 / Test accuracy : 0.820642\n",
      "6 - Training accuracy: 0.816696 / Test accuracy : 0.814957\n",
      "7 - Training accuracy: 0.816856 / Test accuracy : 0.818640\n",
      "\n",
      "Average test accuracy: 0.8167087036592201\n",
      "Variance test accuracy: 0.0\n",
      "Min test accuracy: 0.8167087036592201\n",
      "Max test accuracy: 0.8167087036592201\n",
      "\n",
      "\n",
      "lambda_ = 1\n",
      "gamma = 0.6 \n",
      "\n",
      "0 - Training accuracy: 0.420770 / Test accuracy : 0.417808\n",
      "1 - Training accuracy: 0.421388 / Test accuracy : 0.418448\n",
      "2 - Training accuracy: 0.419226 / Test accuracy : 0.420770\n",
      "3 - Training accuracy: 0.420770 / Test accuracy : 0.419329\n",
      "4 - Training accuracy: 0.419249 / Test accuracy : 0.417327\n",
      "5 - Training accuracy: 0.418345 / Test accuracy : 0.422852\n",
      "6 - Training accuracy: 0.414891 / Test accuracy : 0.422532\n",
      "7 - Training accuracy: 0.420461 / Test accuracy : 0.416126\n",
      "\n",
      "Average test accuracy: 0.41939907118264075\n",
      "Variance test accuracy: 0.0\n",
      "Min test accuracy: 0.41939907118264075\n",
      "Max test accuracy: 0.41939907118264075\n",
      "\n",
      "\n",
      "lambda_ = 1\n",
      "gamma = 0.7 \n",
      "\n",
      "0 - Training accuracy: 0.420770 / Test accuracy : 0.417808\n",
      "1 - Training accuracy: 0.421388 / Test accuracy : 0.418448\n",
      "2 - Training accuracy: 0.419226 / Test accuracy : 0.420770\n",
      "3 - Training accuracy: 0.420770 / Test accuracy : 0.419329\n",
      "4 - Training accuracy: 0.419249 / Test accuracy : 0.417327\n",
      "5 - Training accuracy: 0.418345 / Test accuracy : 0.422852\n",
      "6 - Training accuracy: 0.414891 / Test accuracy : 0.422532\n",
      "7 - Training accuracy: 0.420461 / Test accuracy : 0.416126\n",
      "\n",
      "Average test accuracy: 0.41939907118264075\n",
      "Variance test accuracy: 0.0\n",
      "Min test accuracy: 0.41939907118264075\n",
      "Max test accuracy: 0.41939907118264075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from plots import cross_validation_visualization\n",
    "\n",
    "# Model parameters\n",
    "method = least_squares_GD\n",
    "\n",
    "max_iters = 100\n",
    "seed = 1\n",
    "degree = 7\n",
    "k_fold = 8\n",
    "# lambdas = np.logspace(-4, 0, 30)\n",
    "lambdas = []\n",
    "lambdas.append(1)\n",
    "gammas = np.arange(0.4, 0.8, 0.1)\n",
    "\n",
    "y_pred_final = np.zeros((len(ids_te),1))\n",
    "\n",
    "# For each case (jet = 0, jet = 1, jet = 2,3)\n",
    "for ind, subset_train in enumerate(datasets_train[0:1]):\n",
    "    print(\"\\n\\n For case jet = {} \\n\".format(ind))\n",
    "    x = subset_train[0]\n",
    "    y = subset_train[1]\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    # define lists to store the accuracy of training data and test data\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "\n",
    "    for lambda_ in lambdas:\n",
    "        for gamma in gammas:\n",
    "            print(\"\\nlambda_ = {}\".format(lambda_))\n",
    "            print(\"gamma = {} \\n\".format(gamma))\n",
    "        \n",
    "            param_method = dict(max_iters = max_iters, gamma = gamma, lambda_ = lambda_)\n",
    "\n",
    "            accuracy_train_tmp = []\n",
    "            accuracy_test_tmp = []\n",
    "            w_tmp = []\n",
    "            # cross validation  \n",
    "            for k in range(k_fold):\n",
    "                accuracy_train, accuracy_test, w = cross_validation(y, x, k_indices, k, method = method, **param_method)\n",
    "                accuracy_train_tmp.append(accuracy_train)\n",
    "                accuracy_test_tmp.append(accuracy_test)\n",
    "                w_tmp.append(w)\n",
    "                print(\"%d - Training accuracy: %f / Test accuracy : %f\" % (k, accuracy_train, accuracy_test))\n",
    "\n",
    "            accuracy_train = np.mean(accuracy_train_tmp)\n",
    "            accuracy_test = np.mean(accuracy_test_tmp)\n",
    "\n",
    "            print(\"\\nAverage test accuracy: {}\".format(np.mean(accuracy_test)))\n",
    "            print(\"Variance test accuracy: {}\".format(np.std(accuracy_test)))\n",
    "            print(\"Min test accuracy: {}\".format(np.min(accuracy_test)))\n",
    "            print(\"Max test accuracy: {}\\n\".format(np.max(accuracy_test)))\n",
    "\n",
    "    \n",
    "    \n",
    "    # ****************************************\n",
    "    # ****** Predict for datasets_test *******\n",
    "    # ****************************************\n",
    "    if test == True:\n",
    "        subset_test = datasets_test[ind][0]\n",
    "        _, poly_test = build_model_data(subset_test, datasets_test[ind][1])\n",
    "\n",
    "        y_pred_crt = predict_labels(w_tmp[0], poly_test)   # TODO : for now takes the first w\n",
    "        np.put(y_pred_final, datasets_test[ind][1], y_pred_crt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test == True:\n",
    "    if method == least_squares_GD:\n",
    "        create_csv_submission(ids_te, y_pred_final, '../least_squares_GD.csv')\n",
    "        \n",
    "    elif method == least_squares_SGD:\n",
    "        create_csv_submission(ids_te, y_pred_final, '../least_squares_SGD.csv')\n",
    "        \n",
    "    elif method == least_squares:\n",
    "        create_csv_submission(ids_te, y_pred_final, '../least_squares.csv')\n",
    "        \n",
    "    elif method == ridge_regression:\n",
    "        create_csv_submission(ids_te, y_pred_final, '../ridge_regression.csv')\n",
    "        \n",
    "    elif method == logistic_regression:\n",
    "        create_csv_submission(ids_te, y_pred_final, '../logistic_regression.csv')\n",
    "        \n",
    "    elif method == reg_logistic_regression:\n",
    "        create_csv_submission(ids_te, y_pred_final, '../reg_logistic_regression.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
